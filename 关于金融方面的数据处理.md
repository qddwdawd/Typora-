# 关于数据处理的方法和步骤

- 先导入所需要的包

~~~python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from sklearn.lineae_model import LogisticRegression
from sklearn.linear_model import Ridge
%matplotlib inline
~~~

- 而后进行导包将dta数据转化成csv数据

~~~python
import os
 
def load_large_dta(fname):
    import sys
    reader = pd.read_stata(fname,iterator=True)
    df = pd.DataFrame()
    try:
        chunk = reader.get_chunk(100*1000)
        while len(chunk) > 0:
            df = df.append(chunk, ignore_index=True)
            chunk = reader.get_chunk(100*1000)
            print ('.')
            sys.stdout.flush()
    except (StopIteration, KeyboardInterrupt):
        pass
    print('\nloaded {} rows'.format(len(df)))
    return df
 
def deconde_str(string):
    """
    解码 dta文件防止 乱码
    """
    re = string.encode('latin-1').decode('utf-8')
    return re
 
# example
df_2002_path="./数据.dta"
data=load_large_dta(df_2002_path)
data = pd.DataFrame(data)
data.describe()
~~~

- 进行data.describe()之后再JupyterLab中运行就可以看到处理数据后的结果

由于所处理的数据众多，只能够根据所需要的数据来进行特征量的挑选，以下是挑选了22个特征量。然后是选择了目标量score。

~~~python
feature = ['state','amount','term','gender','age','edu','score',
           'marriage','exp','location','income','house','housed','car','overdue','applys','successs','payoff','card','interest', 'v24','risk']
target = data['score']
x = data[feature]
x.describe()
~~~

- 在JupyterLab中运行，可以看出所选feature的所有信息。

~~~python
pd.set_option('display,max_columns',None)
X.info()
~~~

- 使用pd. set_option来查看所有的列，而后用info显示所有的x的相关信息。我们可以看到共22列，可以看到Dtype。
- 而后我们可以进行柱状图的简单分析，选取其中的几个重要特征参数。

~~~python
fig = plt.figure(figesize=(10,6),dpi = 150)
plt.hist(X['age'])  #将X中的age特征用直方图的形式展现出来
plt.show()
~~~

~~~python
fig = plt.figure(figsize = (10,6),dpi = 150)
plt.hist(X['edu'])
plt.show()
~~~

~~~python
fig = lpt.figure(figsize=(10,6),dpi = 150)
plt.hist(X['amount'])
plt.show()
~~~

~~~python
fig = plt.figure(figsize=(10,6),dpi=150)
plt.hist(X['location'],rwidth=0.5,density=True, facecolor='c', alpha=0.75)
plt.savefig('location.png')
plt.show()
~~~

- 接下来我们要对整体的特征来进行绘图观察，我们可以先进行scipy.stats .probplot的方法来检验样本数据概率分布是否符合正态分布。

~~~python
import seaborn as sns
import scipy
data_3 = X.copy()
train_cols = 4
train_rows = len(data_3.columns)
plt.figure(figsize = (8*train_cols,8*6))
i = 0
for col in data_3.colmuns:
    i+=1
    plt.subplot(6,train_cols,i)
    scipy.stats.probplot(data_3[col],plot = plt)
plt.tight_latout()
plt.show()
plt.clf()
~~~

- 以上为检验是否符合正态分布并且进行了绘图操作，figsize设置(8 * train_cols,8 * 6)是因为下面的 plt.subplot(6,train_cols,i),这行代码的意思是将figsize设置的画布分成6 * train_cols的小画布及有6行train_cols列，后面的i表示第几个小画布。且每个小画布都有8*8大小。
- plt . tight_ lat out()是将画完的图像展示的更加精美，就是自动调节标签的位置。
- 图中的红线表示的是正态分布曲线，蓝线表示的是你统计的数据。

**logistic 回归主要用于处理数值型变 量，当遇到属性变量(如学历、职级、性别等)时只能 通过加入虚拟变量来替代，但客户数据库中大多数 变量都为属性变量，如学历、职级、婚姻、行业、性别、 地区、是否有其他担保品、是否有担保人等，当自变 量存在 J 个水平值，则需要添加 J － 1 个虚拟变量， 假如要把所有的属性变量都转化为虚拟变量，则模 型中要包含几十个甚至更多的自变量，即使是先进 行了变量筛选，剔除其中一些不重要的变量，自变量 的个数仍然很多，如此庞大的自变量个数必然导致 严重的共线性，使参数检验失效，出现较大的偏差。 第四，logistic 回归必须先假定模型符合一定的假 设，如残差服从正态分布、相互独立等，这些在实际 操作中通常难以得到保证,从上面的数据的Q-Q图我们可以看到数据并不符合正太分布，因此其使用logitic回归进行预测效果可能会很差**

#### 那么我们可以先画一个热力图，来表示各特征之间的相关强度

~~~python
df_corr = data.corr()
k=12
cols = df_corr.nlargest(K,'risk')['risk'].index #选出12个线性关系较强的因素(列)
cm = np.corrcoef(data[cols].values.T)
hm = plt.subplots(figsize=(10,10)) #热力值图好像只能用subplots
hm= sns.heatmap(data[cols].corr()，annot=True,square=True)
plt.show()
~~~







